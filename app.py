import streamlit as st
import pandas as pd
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import soundfile as sf
import os
import joblib

# Configuration for Matplotlib to display Chinese characters
plt.rcParams['font.sans-serif'] = ['SimHei']  # Use SimHei for Chinese characters
plt.rcParams['axes.unicode_minus'] = False    # Fix minus sign display

import requests
import speech_recognition as sr
import whisper

# from streamlit_audiorecorder import audiorecorder # Removed external dependency
from utils import extract_features, get_gender_from_pitch, load_data
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Page Config
st.set_page_config(
    page_title="è¯­éŸ³æƒ…æ„Ÿä¸äººå·¥æ™ºèƒ½åˆ†æç³»ç»Ÿ (ä¸“ä¸šç‰ˆ)",
    page_icon="ğŸ™ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for "Beautiful" UI
def apply_custom_css():
    st.markdown("""
    <style>
        /* Global Settings */
        .main {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        /* Sidebar Styling */
        [data-testid="stSidebar"] {
            background-image: linear-gradient(to bottom, #EBF5FC, #D6EAF8);
            color: #2c3e50;
            border-right: 1px solid #AED6F1;
        }
        [data-testid="stSidebar"] .css-17lntkn { 
            color: #2c3e50;
        }
        [data-testid="stSidebar"] h1, [data-testid="stSidebar"] h2, [data-testid="stSidebar"] h3 {
            color: #2c3e50 !important;
        }
        [data-testid="stSidebar"] .stMarkdown, [data-testid="stSidebar"] .stRadio label {
             color: #2c3e50 !important;
        }
        /* Radio Button Base Style */
        [data-testid="stSidebar"] .stRadio label {
            padding: 10px;
            border-radius: 8px;
            margin-bottom: 5px;
            transition: background-color 0.3s;
        }

        /* Hover Effect */
        [data-testid="stSidebar"] .stRadio label:hover {
            background-color: rgba(255, 255, 255, 0.5);
        }

        /* Selected Label Container - Light Blue Background & Left Border */
        [data-testid="stSidebar"] .stRadio label[data-baseweb="radio"][aria-checked="true"] {
            background-color: rgba(33, 150, 243, 0.15) !important;
            border-left: 6px solid #1976D2 !important;
            padding-left: 14px !important; /* Compensate for border width */
        }

        /* Selected Radio Button Circle - Blue Fill */
        [data-testid="stSidebar"] .stRadio label[data-baseweb="radio"][aria-checked="true"] > div:first-child {
            background-color: #1976D2 !important;
            border-color: #1976D2 !important;
            box-shadow: 0 0 5px rgba(25, 118, 210, 0.5);
        }
        
        /* Selected Text - Blue & Bold */
        [data-testid="stSidebar"] .stRadio label[data-baseweb="radio"][aria-checked="true"] p {
            color: #0D47A1 !important;
            font-weight: 800 !important;
            font-size: 1.1em !important;
        }
        
        /* Titles and Headers */
        h1, h2, h3 {
            color: #2c3e50;
            font-weight: 600;
        }
        h1 {
            text-align: center;
            padding-bottom: 20px;
            border-bottom: 2px solid #3498db;
            margin-bottom: 30px;
        }
        
        /* Buttons */
        .stButton>button {
            width: 100%;
            border-radius: 20px;
            height: 3em;
            background: linear-gradient(to right, #4facfe 0%, #00f2fe 100%);
            color: white;
            border: none;
            font-weight: bold;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
        }
        .stButton>button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 8px rgba(0,0,0,0.15);
        }
        
        /* Cards/Containers */
        div.css-1r6slb0 {
            background-color: white;
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
        }
        
        /* Metrics */
        [data-testid="stMetricValue"] {
            font-size: 2rem;
            color: #e74c3c;
        }
        
        /* Tabs */
        .stTabs [data-baseweb="tab-list"] {
            gap: 24px;
        }
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            white-space: pre-wrap;
            background-color: white;
            border-radius: 4px 4px 0px 0px;
            gap: 1px;
            padding-top: 10px;
            padding-bottom: 10px;
        }
        .stTabs [aria-selected="true"] {
            background-color: #E3F2FD;
            color: #1565C0;
            border-bottom: 2px solid #1976D2;
        }
    </style>
    """, unsafe_allow_html=True)

apply_custom_css()

# Constants
DATA_DIR = r"d:\è¯­è¨€ä¿¡å·å¤„ç†\keshe2\archive"
MODEL_PATH = "emotion_model.pkl"
DEEPSEEK_API_KEY = "sk-31baa9eb5d5f4ec78f80d37021f0330c"

# Load Model
@st.cache_resource
def load_emotion_model():
    if os.path.exists(MODEL_PATH):
        return joblib.load(MODEL_PATH)
    return None

model = load_emotion_model()

@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

whisper_model = load_whisper_model()

# Translation Mapping
EMOTION_MAP = {
    "euphoric": "æ„‰æ‚¦",
    "joyfully": "å¿«ä¹",
    "sad": "æ‚²ä¼¤",
    "surprised": "æƒŠè®¶"
}

# Helper Functions
def transcribe_audio(audio_path, language_option="è‡ªåŠ¨æ£€æµ‹ (Auto)"):
    # Map option to code
    lang_code = None
    google_lang = 'zh-CN' # Default fallback
    
    if "Chinese" in language_option:
        lang_code = "zh"
        google_lang = 'zh-CN'
    elif "English" in language_option:
        lang_code = "en"
        google_lang = 'en-US'
        
    try:
        # Use Whisper for better accuracy
        # Load audio with librosa to ensure 16kHz and compatibility
        y, _ = librosa.load(audio_path, sr=16000)
        
        # Transcribe
        if lang_code:
            result = whisper_model.transcribe(y, language=lang_code)
        else:
            result = whisper_model.transcribe(y) # Auto detect
            
        text = result["text"]
        
        # If empty, try Google as fallback
        if not text.strip():
            raise Exception("Whisper returned empty text")
            
        return text
    except Exception as e:
        # Fallback to Google Speech Recognition
        # print(f"Whisper failed: {e}, using Google fallback...")
        r = sr.Recognizer()
        try:
            with sr.AudioFile(audio_path) as source:
                audio_data = r.record(source)
                text = r.recognize_google(audio_data, language=google_lang)
                return text
        except sr.UnknownValueError:
            return "æ— æ³•è¯†åˆ«è¯­éŸ³å†…å®¹"
        except sr.RequestError:
            return "APIè¯·æ±‚å¤±è´¥"
        except Exception as e2:
            return f"Error: {e} | Fallback Error: {e2}"

def call_deepseek(text, emotion_prediction, gender):
    if not text or text.startswith("Error") or text == "æ— æ³•è¯†åˆ«è¯­éŸ³å†…å®¹":
        return "æ— æ³•è¿›è¡ŒAIåˆ†æï¼Œå› ä¸ºè¯­éŸ³è¯†åˆ«å¤±è´¥ã€‚"
    
    url = "https://api.deepseek.com/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
    }
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­éŸ³æƒ…æ„Ÿåˆ†æå¸ˆå’Œå¿ƒç†å’¨è¯¢å¸ˆã€‚
    
    ç”¨æˆ·è¯­éŸ³å†…å®¹ï¼š"{text}"
    ç³»ç»Ÿé¢„æµ‹æƒ…æ„Ÿï¼š{emotion_prediction}
    ç³»ç»Ÿé¢„æµ‹æ€§åˆ«ï¼š{gender}
    
    è¯·ç»“åˆè¯­éŸ³å†…å®¹ã€é¢„æµ‹çš„æƒ…æ„Ÿå’Œæ€§åˆ«ï¼Œè¿›è¡Œæ·±åº¦çš„å¤šæ¨¡æ€åˆ†æï¼š
    1. åˆ†æè¯´è¯äººçš„å½“å‰æƒ…ç»ªçŠ¶æ€ã€‚
    2. æ¨æµ‹æ½œåœ¨çš„å¿ƒç†ç‰¹å¾æˆ–å‹åŠ›æ¥æºã€‚
    3. ç»™å‡ºé’ˆå¯¹æ€§çš„æ²Ÿé€šå»ºè®®æˆ–å¿ƒç†è°ƒèŠ‚å»ºè®®ã€‚
    """
    
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­éŸ³æƒ…æ„Ÿåˆ†æå¸ˆå’Œå¿ƒç†å’¨è¯¢å¸ˆã€‚"},
            {"role": "user", "content": prompt}
        ],
        "stream": False
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            return f"DeepSeek API Error: {response.status_code} - {response.text}"
    except Exception as e:
        return f"Connection Error: {e}"

def plot_waveform(y, sr):
    fig, ax = plt.subplots(figsize=(10, 3))
    librosa.display.waveshow(y, sr=sr, ax=ax)
    ax.set_title("æ—¶åŸŸæ³¢å½¢å›¾(Waveform)")
    plt.tight_layout()
    return fig

def plot_spectrogram(y, sr):
    fig, ax = plt.subplots(figsize=(10, 3))
    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
    img = librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr, ax=ax)
    fig.colorbar(img, ax=ax, format="%+2.0f dB")
    ax.set_title("é¢‘è°±å›¾(Spectrogram)")
    plt.tight_layout()
    return fig

def plot_mel_spectrogram(y, sr):
    fig, ax = plt.subplots(figsize=(10, 3))
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
    S_dB = librosa.power_to_db(S, ref=np.max)
    img = librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000, ax=ax)
    fig.colorbar(img, ax=ax, format='%+2.0f dB')
    ax.set_title("æ¢…å°”é¢‘è°±å›¾(Mel-Spectrogram)")
    plt.tight_layout()
    return fig

def plot_chroma(y, sr):
    fig, ax = plt.subplots(figsize=(10, 3))
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    img = librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', ax=ax)
    fig.colorbar(img, ax=ax)
    ax.set_title("è‰²åº¦å›¾(Chromagram)")
    plt.tight_layout()
    return fig

def plot_mfcc(y, sr):
    fig, ax = plt.subplots(figsize=(10, 3))
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
    img = librosa.display.specshow(mfcc, x_axis='time', ax=ax)
    fig.colorbar(img, ax=ax)
    ax.set_title("MFCCç‰¹å¾çƒ­åŠ›å›¾")
    plt.tight_layout()
    return fig

# Sidebar
with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/4712/4712035.png", width=100)
    st.title("ğŸ™ï¸ è¯­éŸ³åˆ†æ Pro")
    st.markdown("---")
    st.info("ğŸ’¡ åŸºäºå¢å¼ºç‰¹å¾ä¸å¤šæ¨¡å‹èåˆçš„ä¸“ä¸šåˆ†æå¹³å°")
    
    st.markdown("### ğŸ§­ å¯¼èˆªèœå•")
    page = st.radio("Go to", ["æ•°æ®é›†æ¦‚è§ˆä¸åˆ†å¸ƒ", "è¯­éŸ³æ·±åº¦åˆ†æä¸AIè¯Šæ–­"], label_visibility="collapsed")
    
    st.markdown("---")
    st.markdown("### âš™ï¸ ç³»ç»ŸçŠ¶æ€")
    st.caption(f"âœ… æ¨¡å‹çŠ¶æ€: {'å·²åŠ è½½' if model else 'æœªåŠ è½½'}")
    st.caption(f"âœ… Whisper: {'å·²åŠ è½½' if whisper_model else 'æœªåŠ è½½'}")
    
    st.markdown("---")

if page == "æ•°æ®é›†æ¦‚è§ˆä¸åˆ†å¸ƒ":
    st.title("ğŸ“Š æ•°æ®é›†é«˜çº§åˆ†æ")
    
    if os.path.exists(DATA_DIR):
        with st.spinner("æ­£åœ¨åŠ è½½å’Œåˆ†ææ•°æ®é›†..."):
            df = load_data(DATA_DIR)
            st.write(f"**æ€»æ–‡ä»¶æ•°:** {len(df)}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("#### æƒ…æ„Ÿç±»åˆ«åˆ†å¸ƒ")
                st.bar_chart(df['label'].value_counts())
            
            with col2:
                st.markdown("#### æ•°æ®é¢„è§ˆ")
                st.dataframe(df.head(10), use_container_width=True)
            
            st.markdown("### ğŸ§  æ·±åº¦ AI ç®—æ³•å…¨æ™¯åˆ†æ")
            st.write("é›†æˆ PCA/t-SNE é™ç»´ã€K-Means/DBSCAN èšç±»ã€å­¤ç«‹æ£®æ—å¼‚å¸¸æ£€æµ‹ä»¥åŠå¤šæ¨¡å‹å¯¹æ¯”åˆ†æã€‚")
            
            # Initialize session state for AI analysis
            if 'ai_analysis_data' not in st.session_state:
                st.session_state.ai_analysis_data = None

            if st.button("å¯åŠ¨å…¨ç®—æ³•å¼•æ“"):
                features_list = []
                labels_list = []
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                total = len(df)
                
                # 1. Feature Extraction
                status_text.text("æ­£åœ¨æå–éŸ³é¢‘ç‰¹å¾ (MFCC, Chroma, Mel, Contrast)...")
                for i, row in df.iterrows():
                    feat = extract_features(file_path=row['path'])
                    if feat is not None:
                        features_list.append(feat)
                        labels_list.append(row['label'])
                    progress_bar.progress((i + 1) / total)
                
                if features_list:
                    X = np.array(features_list)
                    y_labels = np.array(labels_list)
                    
                    # Standardization
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    
                    # Save to session state
                    st.session_state.ai_analysis_data = {
                        'X_scaled': X_scaled,
                        'y_labels': y_labels
                    }
                    st.success("ç‰¹å¾æå–å®Œæˆï¼AI å¼•æ“å·²å°±ç»ªã€‚")
                else:
                    st.error("æœªèƒ½æå–åˆ°æœ‰æ•ˆç‰¹å¾ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ã€‚")

            # Render analysis if data is available in session state
            if st.session_state.ai_analysis_data is not None:
                data = st.session_state.ai_analysis_data
                X_scaled = data['X_scaled']
                y_labels = data['y_labels']
                
                # Re-calculate X for visualizations that need original shape if needed, 
                # but we stored scaled X.
                # Dimensionality Reduction needs to be re-run or stored? 
                # It's fast enough to re-run for visualization usually, 
                # but caching PCA/t-SNE results would be better if dataset is large.
                # For now, we re-run them to keep code simple, or we could store them too.
                # Given dataset size (likely small/medium), re-running is okay.

                # --- Tab Layout for Analysis ---
                st.markdown("---")
                tab_dim, tab_cluster, tab_anomaly, tab_models, tab_importance = st.tabs([
                    "ğŸŒŒ é™ç»´å¯è§†åŒ–", "ğŸ§© èšç±»åˆ†æ", "ğŸ” å¼‚å¸¸æ£€æµ‹", "âš”ï¸ æ¨¡å‹ç«æŠ€åœº", "ğŸ”‘ ç‰¹å¾è§£å¯†"
                ])
                
                # Pre-calculate PCA for use in multiple tabs
                pca = PCA(n_components=2)
                X_pca = pca.fit_transform(X_scaled)
                pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
                pca_df['Emotion'] = y_labels
                pca_df['Emotion_CN'] = pca_df['Emotion'].map(lambda x: EMOTION_MAP.get(x, x))

                # 1. Dimensionality Reduction (PCA & t-SNE)
                with tab_dim:
                    col1, col2 = st.columns(2)
                    
                    # PCA
                    with col1:
                        st.subheader("PCA çº¿æ€§é™ç»´")
                        fig_pca = px.scatter(pca_df, x='PC1', y='PC2', color='Emotion_CN', 
                                         title='PCA åˆ†å¸ƒå›¾',
                                         hover_data=['Emotion'],
                                         template='plotly_white',
                                         color_discrete_sequence=px.colors.qualitative.Bold)
                        st.plotly_chart(fig_pca, use_container_width=True)
                        st.info(f"PCA è§£é‡Šæ–¹å·®: {np.sum(pca.explained_variance_ratio_):.2%}")

                    # t-SNE
                    with col2:
                        st.subheader("t-SNE éçº¿æ€§æµå½¢å­¦ä¹ ")
                        # t-SNE can be slow, maybe cache it in session_state too if needed.
                        # For now, calculate it.
                        if 'tsne_df' not in st.session_state:
                             n_samples = X_scaled.shape[0]
                             perplexity_val = min(30, n_samples - 1) 
                             tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val)
                             X_tsne = tsne.fit_transform(X_scaled)
                             tsne_df = pd.DataFrame(data=X_tsne, columns=['Dim1', 'Dim2'])
                             tsne_df['Emotion_CN'] = [EMOTION_MAP.get(label, label) for label in y_labels]
                             st.session_state.tsne_df = tsne_df
                        else:
                             tsne_df = st.session_state.tsne_df
                        
                        fig_tsne = px.scatter(tsne_df, x='Dim1', y='Dim2', color='Emotion_CN',
                                          title='t-SNE åˆ†å¸ƒå›¾',
                                          template='plotly_white',
                                          color_discrete_sequence=px.colors.qualitative.Bold)
                        st.plotly_chart(fig_tsne, use_container_width=True)
                        st.caption("t-SNE èƒ½æ›´å¥½åœ°å±•ç¤ºæ•°æ®çš„å±€éƒ¨ç»“æ„å’Œç±»åˆ«åˆ†ç¦»åº¦ã€‚")

                # 2. Clustering (K-Means & DBSCAN)
                with tab_cluster:
                    st.subheader("æ— ç›‘ç£èšç±»åˆ†æ (Unsupervised Clustering)")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("#### K-Means (Kå‡å€¼èšç±»)")
                        n_clusters = len(np.unique(y_labels))
                        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                        clusters_km = kmeans.fit_predict(X_scaled)
                        
                        cluster_df = pca_df.copy()
                        cluster_df['Cluster'] = clusters_km.astype(str)
                        
                        fig_km = px.scatter(cluster_df, x='PC1', y='PC2', color='Cluster', symbol='Emotion_CN',
                                         title=f'K-Means ç»“æœ (K={n_clusters})',
                                         template='plotly_white')
                        st.plotly_chart(fig_km, use_container_width=True)
                        st.metric("K-Means è½®å»“ç³»æ•°", f"{silhouette_score(X_scaled, clusters_km):.3f}")

                    with col2:
                        st.markdown("#### DBSCAN (å¯†åº¦èšç±»)")
                        # DBSCAN parameters usually need tuning
                        dbscan = DBSCAN(eps=5, min_samples=3)
                        clusters_db = dbscan.fit_predict(X_scaled)
                        
                        cluster_df_db = pca_df.copy()
                        cluster_df_db['Cluster'] = clusters_db.astype(str)
                        
                        fig_db = px.scatter(cluster_df_db, x='PC1', y='PC2', color='Cluster', symbol='Emotion_CN',
                                         title='DBSCAN ç»“æœ (è‡ªåŠ¨å‘ç°ç°‡)',
                                         template='plotly_white')
                        st.plotly_chart(fig_db, use_container_width=True)
                        n_noise = list(clusters_db).count(-1)
                        st.metric("DBSCAN å‘ç°çš„ç°‡æ•°é‡", f"{len(set(clusters_db)) - (1 if -1 in clusters_db else 0)}")
                        st.caption(f"æ³¨: æ ‡ç­¾ä¸º -1 çš„ç‚¹è¢«è§†ä¸ºå™ªå£°ç‚¹ (å…± {n_noise} ä¸ª)")

                # 3. Anomaly Detection (Isolation Forest)
                with tab_anomaly:
                    st.subheader("å¼‚å¸¸æ£€æµ‹ (Anomaly Detection)")
                    st.write("ä½¿ç”¨å­¤ç«‹æ£®æ— (Isolation Forest) ç®—æ³•è¯†åˆ«æ•°æ®é›†ä¸­çš„å¼‚å¸¸æ ·æœ¬æˆ–ç¦»ç¾¤ç‚¹ã€‚")
                    
                    contamination = st.slider("é¢„è®¡å¼‚å¸¸æ¯”ä¾‹ (Contamination)", 0.01, 0.20, 0.05, 0.01)
                    
                    iso_forest = IsolationForest(contamination=contamination, random_state=42)
                    outliers = iso_forest.fit_predict(X_scaled)
                    
                    anomaly_df = pca_df.copy()
                    anomaly_df['Type'] = np.where(outliers == -1, 'å¼‚å¸¸ (Anomaly)', 'æ­£å¸¸ (Normal)')
                    
                    fig_anom = px.scatter(anomaly_df, x='PC1', y='PC2', color='Type', symbol='Emotion_CN',
                                      title=f'å­¤ç«‹æ£®æ—å¼‚å¸¸æ£€æµ‹ç»“æœ (å¼‚å¸¸æ¯”ä¾‹: {contamination})',
                                      color_discrete_map={'å¼‚å¸¸ (Anomaly)': 'red', 'æ­£å¸¸ (Normal)': 'lightgrey'},
                                      template='plotly_white',
                                      hover_data=['Emotion_CN'])
                    st.plotly_chart(fig_anom, use_container_width=True)
                    
                    if -1 in outliers:
                        st.warning(f"æ£€æµ‹åˆ° {list(outliers).count(-1)} ä¸ªæ½œåœ¨çš„å¼‚å¸¸æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬å¯èƒ½åŒ…å«æ‚éŸ³æˆ–æ ‡è®°é”™è¯¯ã€‚")

                # 4. Model Comparison
                with tab_models:
                    st.subheader("æ¨¡å‹ç«æŠ€åœº (Model Comparison)")
                    st.write("å¯¹æ¯”ä¸åŒæœºå™¨å­¦ä¹ ç®—æ³•åœ¨è¯¥æ•°æ®é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½ (80% è®­ç»ƒ, 20% æµ‹è¯•)ã€‚")
                    
                    if st.button("å¼€å§‹æ¨¡å‹å¯¹å†³"):
                        with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œè¯„ä¼°..."):
                            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_labels, test_size=0.2, random_state=42)
                            
                            models = {
                                "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
                                "SVM (RBF Kernel)": SVC(kernel='rbf', probability=True),
                                "KNN (k=5)": KNeighborsClassifier(n_neighbors=5),
                                "Naive Bayes": GaussianNB()
                            }
                            
                            results = []
                            
                            model_cols = st.columns(len(models))
                            
                            for idx, (name, clf) in enumerate(models.items()):
                                clf.fit(X_train, y_train)
                                y_pred = clf.predict(X_test)
                                acc = accuracy_score(y_test, y_pred)
                                results.append({"Model": name, "Accuracy": acc})
                                with model_cols[idx]:
                                    st.metric(name, f"{acc:.2%}")
                            
                            res_df = pd.DataFrame(results).sort_values(by="Accuracy", ascending=False)
                            fig_res = px.bar(res_df, x="Accuracy", y="Model", orientation='h', 
                                         title="å„æ¨¡å‹å‡†ç¡®ç‡æ’è¡Œæ¦œ", color="Accuracy",
                                         color_continuous_scale="Blues", text_auto='.2%')
                            st.plotly_chart(fig_res, use_container_width=True)

                # 5. Feature Importance
                with tab_importance:
                    st.subheader("ç‰¹å¾é‡è¦æ€§è§£å¯†")
                    rf = RandomForestClassifier(n_estimators=100, random_state=42)
                    rf.fit(X_scaled, y_labels)
                    
                    importances = rf.feature_importances_
                    feature_groups = {
                        "MFCC (å€’è°±ç³»æ•°)": importances[0:80].sum(),
                        "Chroma (è‰²åº¦)": importances[80:104].sum(),
                        "Mel Spectrogram (æ¢…å°”é¢‘è°±)": importances[104:360].sum(),
                        "Spectral Contrast (å…‰è°±å¯¹æ¯”åº¦)": importances[360:].sum()
                    }
                    
                    imp_df = pd.DataFrame(list(feature_groups.items()), columns=['Feature Type', 'Importance'])
                    imp_df = imp_df.sort_values(by='Importance', ascending=False)
                    
                    fig_imp = px.bar(imp_df, x='Importance', y='Feature Type', orientation='h',
                                 title='å„ç±»è¯­éŸ³ç‰¹å¾çš„é‡è¦æ€§è´¡çŒ®åº¦',
                                 color='Importance',
                                 color_continuous_scale='Viridis')
                    st.plotly_chart(fig_imp, use_container_width=True)
            
            if st.button("æ¸…ç©ºåˆ†æç¼“å­˜"):
                del st.session_state.ai_analysis_data
                if 'tsne_df' in st.session_state:
                    del st.session_state.tsne_df
                st.rerun()

elif page == "è¯­éŸ³æ·±åº¦åˆ†æä¸AIè¯Šæ–­":
    st.title("ğŸ§  è¯­éŸ³æ·±åº¦åˆ†æä¸AIè¯Šæ–­")
    st.markdown("ä¸Šä¼ éŸ³é¢‘æˆ–å®æ—¶å½•éŸ³ï¼Œç³»ç»Ÿå°†è¿›è¡Œå¤šç»´åº¦ä¿¡å·å¤„ç†ã€æƒ…æ„Ÿ/æ€§åˆ«è¯†åˆ«ï¼Œå¹¶åˆ©ç”¨DeepSeek AIè¿›è¡Œæ·±åº¦å¿ƒç†åˆ†æã€‚")
    
    input_method = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼", ["ä¸Šä¼ æ–‡ä»¶", "å®æ—¶å½•éŸ³"], horizontal=True)
    
    audio_path = None
    
    if input_method == "ä¸Šä¼ æ–‡ä»¶":
        uploaded_file = st.file_uploader("ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (WAV, MP3)", type=["wav", "mp3", "mpeg"])
        if uploaded_file:
            with open("temp_upload.wav", "wb") as f:
                f.write(uploaded_file.getbuffer())
            audio_path = "temp_upload.wav"
            st.audio(audio_path)
            
    elif input_method == "å®æ—¶å½•éŸ³":
        st.write("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹å½•éŸ³ï¼š")
        # Use native st.audio_input (requires Streamlit >= 1.40)
        audio_buffer = st.audio_input("è¯·å½•éŸ³")
        
        if audio_buffer:
            # Save the recorded file
            with open("temp_record.wav", "wb") as f:
                f.write(audio_buffer.getbuffer())
            audio_path = "temp_record.wav"
            st.success("å½•éŸ³å®Œæˆï¼")
    
    if audio_path:
        # Language Selection
        st.markdown("### ğŸ› ï¸ åˆ†æè®¾ç½®")
        language_option = st.selectbox(
            "é€‰æ‹©è¯­éŸ³è¯­è¨€ (Select Language)",
            ["è‡ªåŠ¨æ£€æµ‹ (Auto)", "ä¸­æ–‡ (Chinese)", "è‹±æ–‡ (English)"],
            index=0,
            help="é€‰æ‹©éŸ³é¢‘çš„ä¸»è¦è¯­è¨€ï¼Œ'è‡ªåŠ¨æ£€æµ‹'é€šå¸¸æ•ˆæœæœ€å¥½ï¼Œä½†åœ¨æ‚éŸ³è¾ƒå¤šæ—¶æŒ‡å®šè¯­è¨€æ›´å‡†ç¡®ã€‚"
        )

        if st.button("å¼€å§‹å…¨ç»´æ™ºèƒ½åˆ†æ"):
            with st.spinner("æ­£åœ¨è¿›è¡Œå¤šç»´ä¿¡å·å¤„ç†ã€ç‰¹å¾æå–å’ŒAIæ¨ç†..."):
                # Load Audio
                y, sample_rate = librosa.load(audio_path)
                
                # --- Section 1: Traditional Signal Processing Visualization ---
                st.header("1. å¤šç»´ä¿¡å·å¯è§†åŒ–å›¾è°±")
                
                tab1, tab2, tab3, tab4, tab5 = st.tabs(["æ³¢å½¢å›¾", "è¯­è°±å›¾", "æ¢…å°”é¢‘è°±", "è‰²åº¦å›¾", "MFCCçƒ­åŠ›å›¾"])
                
                with tab1:
                    st.pyplot(plot_waveform(y, sample_rate))
                with tab2:
                    st.pyplot(plot_spectrogram(y, sample_rate))
                with tab3:
                    st.pyplot(plot_mel_spectrogram(y, sample_rate))
                    st.caption("æ¢…å°”é¢‘è°±å›¾æ›´ç¬¦åˆäººè€³å¬è§‰ç‰¹æ€§ï¼Œå±•ç¤ºäº†ä¸åŒé¢‘ç‡ä¸Šçš„èƒ½é‡åˆ†å¸ƒã€‚")
                with tab4:
                    st.pyplot(plot_chroma(y, sample_rate))
                    st.caption("è‰²åº¦å›¾å±•ç¤ºäº†éŸ³é¢‘ä¸­çš„éŸ³é«˜ç±»åˆ«ï¼ˆC, C#, D...ï¼‰ï¼Œæœ‰åŠ©äºåˆ†æéŸ³è°ƒç‰¹å¾ã€‚")
                with tab5:
                    st.pyplot(plot_mfcc(y, sample_rate))
                    st.caption("MFCCï¼ˆæ¢…å°”é¢‘ç‡å€’è°±ç³»æ•°ï¼‰æ˜¯è¯­éŸ³è¯†åˆ«ä¸­æœ€æ ¸å¿ƒçš„ç‰¹å¾ã€‚")
                
                # --- Section 2: Model Prediction ---
                st.header("2. æ™ºèƒ½è¯†åˆ«ç»“æœ")
                col_a, col_b = st.columns(2)
                
                # Gender
                gender = get_gender_from_pitch(audio_path)
                col_a.metric("è¯†åˆ«æ€§åˆ«", gender, delta="åŸºäºåŸºé¢‘åˆ†æ")
                
                # Emotion (Model)
                prediction_cn = "æœªçŸ¥"
                if model:
                    features = extract_features(audio_path)
                    if features is not None:
                        # Ensure features shape matches model input (model expects 2D array)
                        prediction = model.predict([features])[0]
                        
                        # Translate prediction to Chinese
                        prediction_cn = EMOTION_MAP.get(prediction, prediction)
                        
                        # Get probability if supported
                        if hasattr(model, "predict_proba"):
                            proba = model.predict_proba([features])[0]
                            max_prob = np.max(proba)
                            col_b.metric("è¯†åˆ«æƒ…æ„Ÿ", prediction_cn, delta=f"ç½®ä¿¡åº¦: {max_prob:.2%}")
                            
                            # Show prob chart
                            classes = model.classes_
                            # Map classes to CN
                            classes_cn = [EMOTION_MAP.get(c, c) for c in classes]
                            prob_df = pd.DataFrame({"æƒ…æ„Ÿ": classes_cn, "æ¦‚ç‡": proba})
                            st.bar_chart(prob_df.set_index("æƒ…æ„Ÿ"))
                        else:
                            col_b.metric("è¯†åˆ«æƒ…æ„Ÿ", prediction_cn)
                    else:
                        col_b.metric("è¯†åˆ«æƒ…æ„Ÿ", "ç‰¹å¾æå–å¤±è´¥")
                else:
                    col_b.error("æ¨¡å‹æœªåŠ è½½")
                
                # --- Section 3: AI Analysis (DeepSeek) ---
                st.header("3. DeepSeek AI æ·±åº¦å¿ƒç†æŠ¥å‘Š")
                
                # Transcribe
                text = transcribe_audio(audio_path, language_option)
                st.info(f"**è¯­éŸ³è½¬æ–‡å­—å†…å®¹:** {text}")
                
                if text and text != "æ— æ³•è¯†åˆ«è¯­éŸ³å†…å®¹":
                    analysis = call_deepseek(text, prediction_cn, gender)
                    st.success("**AI å¿ƒç†åˆ†æä¸“å®¶æŠ¥å‘Š:**")
                    st.markdown(analysis)
                else:
                    st.warning("æœªèƒ½è¯†åˆ«å‡ºæœ‰æ•ˆè¯­éŸ³å†…å®¹ï¼Œæ— æ³•è¿›è¡ŒAIæ·±åº¦åˆ†æã€‚è¯·å°è¯•æ¸…æ™°è¯´è¯ã€‚")

# Cleanup temp files
# (Optional: In a real app, use tempfile module)
